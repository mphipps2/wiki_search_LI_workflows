{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Display the complete contents of dataframe cells.\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "import wikipedia\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from typing import Dict\n",
    "from typing import Any, List, Union\n",
    "\n",
    "from llama_index.core.agent.react import ReActChatFormatter, ReActOutputParser\n",
    "from llama_index.core.agent.react.types import (\n",
    "    ActionReasoningStep,\n",
    "    ObservationReasoningStep,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import (\n",
    "    OTLPSpanExporter as HTTPSpanExporter,\n",
    ")\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EvalGenPrompt(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "def generate_eval_prompt(prompt: str) -> EvalGenPrompt:\n",
    "    \"\"\"Generates a concise, markdown-formatted evaluation dataset generation prompt.\"\"\"\n",
    "    return EvalGenPrompt(prompt=prompt)\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=generate_eval_prompt)\n",
    "\n",
    "# Now use the LLM to call this function\n",
    "llm = OpenAI(model=\"gpt-4o\", strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = \"\"\"\n",
    "'Who is the president?'\n",
    "'Who scored the most goals in the European Champion's League in 2020?'\n",
    "'articles similar to the article on 'Philosophy'?'\n",
    "\"the school of athens\"\n",
    "\"css flex\"\n",
    "\"\"\"\n",
    "\n",
    "user_unstructured_prompt = f\"\"\"\n",
    "Your task is to clean up the following unstructured prompt into a well structured, markdown formatted prompt \n",
    "that I can pass to another LLM for synthetic data generation. The prompt should be concise \n",
    "but clear and unambiguous. \n",
    "It should include the following sections: Task, Context, Instructions, and Example Queries.\n",
    "It should respond by calling the tool assigned to it.\n",
    "The task is to generate a dataset of queries that can be used to evaluate the \n",
    "performance of a wikipedia-based agentic search engine.\n",
    "The context is that the user queries are generally of three types: 1) direct query (eg: Who is the \n",
    "president?), 2) return articles similar to a given article (eg: articles similar to 'Philosophy'), \n",
    "3) return a summary of a given subect (eg: the school of athens). The queries generated\n",
    "should cover each of these types and be similar in theme to the user query examples provided below but \n",
    "they should be worded differently and may include typos, capitalization differences, etc. They should be\n",
    "designed to sample a realistic distribution of queries that a user could answer.\n",
    "It should also generate a few out of distribution queries that are not of these types or difficult \n",
    "to answer in some way. \n",
    "\n",
    "user query examples:\n",
    "{examples}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = user_unstructured_prompt\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    [tool],\n",
    "    prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Task\n",
      "Generate a dataset of queries to evaluate the performance of a Wikipedia-based agentic search engine.\n",
      "\n",
      "## Context\n",
      "User queries are generally of three types:\n",
      "1. Direct query (e.g., Who is the president?)\n",
      "2. Return articles similar to a given article (e.g., articles similar to 'Philosophy')\n",
      "3. Return a summary of a given subject (e.g., the school of Athens)\n",
      "\n",
      "The queries generated should cover each of these types and be similar in theme to the user query examples provided below but should be worded differently and may include typos, capitalization differences, etc. They should be designed to sample a realistic distribution of queries that a user could ask. Additionally, generate a few out-of-distribution queries that are not of these types or are difficult to answer in some way.\n",
      "\n",
      "## Instructions\n",
      "1. Generate queries that fit into the three specified types.\n",
      "2. Ensure the queries are similar in theme to the provided examples but are worded differently.\n",
      "3. Include variations such as typos and capitalization differences.\n",
      "4. Generate a few out-of-distribution queries that do not fit the specified types or are difficult to answer.\n",
      "\n",
      "## Example Queries\n",
      "- 'Who is the president?'\n",
      "- 'Who scored the most goals in the European Champion's League in 2020?'\n",
      "- 'articles similar to the article on 'Philosophy'?'\n",
      "- 'the school of athens'\n",
      "- 'css flex'\n"
     ]
    }
   ],
   "source": [
    "for s in response.sources:\n",
    "    tool_output = s.raw_output\n",
    "    if isinstance(tool_output, EvalGenPrompt):\n",
    "        print(tool_output.prompt)\n",
    "\n",
    "        eval_gen_prompt = tool_output.prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries_to_generate = 20\n",
    "\n",
    "eval_gen_prompt += f\"\\n\\n Number of queries to generate: {n_queries_to_generate}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Task\n",
      "Generate a dataset of queries to evaluate the performance of a Wikipedia-based agentic search engine.\n",
      "\n",
      "## Context\n",
      "User queries are generally of three types:\n",
      "1. Direct query (e.g., Who is the president?)\n",
      "2. Return articles similar to a given article (e.g., articles similar to 'Philosophy')\n",
      "3. Return a summary of a given subject (e.g., the school of Athens)\n",
      "\n",
      "The queries generated should cover each of these types and be similar in theme to the user query examples provided below but should be worded differently and may include typos, capitalization differences, etc. They should be designed to sample a realistic distribution of queries that a user could ask. Additionally, generate a few out-of-distribution queries that are not of these types or are difficult to answer in some way.\n",
      "\n",
      "## Instructions\n",
      "1. Generate queries that fit into the three specified types.\n",
      "2. Ensure the queries are similar in theme to the provided examples but are worded differently.\n",
      "3. Include variations such as typos and capitalization differences.\n",
      "4. Generate a few out-of-distribution queries that do not fit the specified types or are difficult to answer.\n",
      "\n",
      "## Example Queries\n",
      "- 'Who is the president?'\n",
      "- 'Who scored the most goals in the European Champion's League in 2020?'\n",
      "- 'articles similar to the article on 'Philosophy'?'\n",
      "- 'the school of athens'\n",
      "- 'css flex'\n",
      "\n",
      " Number of queries to generate: 20.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(eval_gen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(BaseModel):\n",
    "    query: str\n",
    "\n",
    "def generate_eval_dataset(query: str) -> EvalDataset:\n",
    "    \"\"\"Generates an evaluation dataset with a single query.\"\"\"\n",
    "    return EvalDataset(query=query)\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=generate_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o\", strict=True)\n",
    "prompt = eval_gen_prompt\n",
    "# query = \"Give me articles similar to 'Philosophy'\"\n",
    "# Call predict_and_call with both tools\n",
    "response = llm.predict_and_call(\n",
    "    [tool],\n",
    "    prompt,\n",
    "    allow_parallel_tool_calls=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the current Prime Minister?\n",
      "Who won the Nobel Prize in Literature in 2021?\n",
      "articles like the one on 'Metaphysics'\n",
      "the theory of relativity\n",
      "articles similar to 'Quantum Mechanics'\n",
      "Who is the CEO of Tesla?\n",
      "articles related to 'Artificial Intelligence'\n",
      "the history of the Roman Empire\n",
      "Who discovered penicillin?\n",
      "articles like 'Renaissance Art'\n",
      "the life of Albert Einstein\n",
      "Who is the author of '1984'?\n",
      "articles similar to 'Existentialism'\n",
      "the causes of World War II\n",
      "Who painted the Mona Lisa?\n",
      "articles like 'Modern Architecture'\n",
      "the principles of democracy\n",
      "Who is the founder of Microsoft?\n",
      "articles related to 'Climate Change'\n",
      "the impact of the Industrial Revolution\n"
     ]
    }
   ],
   "source": [
    "eval_queries = []\n",
    "for s in response.sources:\n",
    "    tool_output = s.raw_output\n",
    "    if isinstance(tool_output, list) and all(isinstance(item, EvalDataset) for item in tool_output):\n",
    "        for result in tool_output:\n",
    "            print(f\"Query: {result.query}\")\n",
    "            eval_queries.append(result.query)\n",
    "\n",
    "    if isinstance(tool_output, EvalDataset):\n",
    "        print(tool_output.query)\n",
    "\n",
    "        eval_queries.append(tool_output.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is the current Prime Minister?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who won the Nobel Prize in Literature in 2021?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>articles like the one on 'Metaphysics'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the theory of relativity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>articles similar to 'Quantum Mechanics'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            query\n",
       "0              Who is the current Prime Minister?\n",
       "1  Who won the Nobel Prize in Literature in 2021?\n",
       "2          articles like the one on 'Metaphysics'\n",
       "3                        the theory of relativity\n",
       "4         articles similar to 'Quantum Mechanics'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_df = pd.DataFrame(eval_queries, columns=[\"query\"])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "\"Who is the president?\",\n",
    "\"Who scored the most goals in the European Champion's League in 2020?\",\n",
    "\"articles similar to the article on 'Philosophy'?\",\n",
    "\"the school of athens\",\n",
    "\"css flex\"\n",
    "]\n",
    "\n",
    "test_examples_df = pd.DataFrame(test_examples, columns=[\"query\"])\n",
    "\n",
    "# Concatenate the new DataFrame with the existing test_df\n",
    "test_df = pd.concat([test_examples_df, test_df], ignore_index=True)\n",
    "test_df.head()\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "test_df.to_csv(\"../data/evaluation/test_queries.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is the president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who scored the most goals in the European Cham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>articles similar to the article on 'Philosophy'?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the school of athens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>css flex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query\n",
       "0                              Who is the president?\n",
       "1  Who scored the most goals in the European Cham...\n",
       "2   articles similar to the article on 'Philosophy'?\n",
       "3                               the school of athens\n",
       "4                                           css flex"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DataFrame from the CSV file\n",
    "test_df = pd.read_csv(\"../data/evaluation/test_queries.csv\", index_col=0)\n",
    "\n",
    "# Display the first few rows of the loaded DataFrame\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to Phoenix Arize (Traceability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add Phoenix API Key for tracing\n",
    "# PHOENIX_API_KEY = \"<YOUR-PHOENIX-API-KEY>\"\n",
    "# os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "\n",
    "# Add Phoenix\n",
    "span_phoenix_processor = SimpleSpanProcessor(\n",
    "    # HTTPSpanExporter(endpoint=\"https://app.phoenix.arize.com/v1/traces\")\n",
    "    HTTPSpanExporter(endpoint=\"http://0.0.0.0:6006/v1/traces\")\n",
    ")\n",
    "\n",
    "# Add them to the tracer\n",
    "tracer_provider = trace_sdk.TracerProvider()\n",
    "tracer_provider.add_span_processor(span_processor=span_phoenix_processor)\n",
    "\n",
    "# Instrument the application\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PrepEvent(Event):\n",
    "    pass\n",
    "\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReActAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: Union[LLM, None] = None,\n",
    "        tools: Union[list[BaseTool], None] = None,\n",
    "        extra_context: Union[str, None] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm or OpenAI()\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.formatter = ReActChatFormatter(context=extra_context or \"\")\n",
    "        self.output_parser = ReActOutputParser()\n",
    "        self.sources = []\n",
    "\n",
    "    @step\n",
    "    async def new_user_msg(self, ctx: Context, ev: StartEvent) -> PrepEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # clear current reasoning\n",
    "        await ctx.set(\"current_reasoning\", [])\n",
    "\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def prepare_chat_history(\n",
    "        self, ctx: Context, ev: PrepEvent\n",
    "    ) -> InputEvent:\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        current_reasoning = await ctx.get(\"current_reasoning\", default=[])\n",
    "        llm_input = self.formatter.format(\n",
    "            self.tools, chat_history, current_reasoning=current_reasoning\n",
    "        )\n",
    "        return InputEvent(input=llm_input)\n",
    "\n",
    "    @step\n",
    "    async def handle_llm_input(\n",
    "        self, ctx: Context, ev: InputEvent\n",
    "    ) -> Union[ToolCallEvent, StopEvent, PrepEvent]:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        response = await self.llm.achat(chat_history)\n",
    "\n",
    "        try:\n",
    "            reasoning_step = self.output_parser.parse(response.message.content)\n",
    "            (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                reasoning_step\n",
    "            )\n",
    "            if reasoning_step.is_done:\n",
    "                self.memory.put(\n",
    "                    ChatMessage(\n",
    "                        role=\"assistant\", content=reasoning_step.response\n",
    "                    )\n",
    "                )\n",
    "                return StopEvent(\n",
    "                    result={\n",
    "                        \"response\": reasoning_step.response,\n",
    "                        \"sources\": [*self.sources],\n",
    "                        \"reasoning\": await ctx.get(\n",
    "                            \"current_reasoning\", default=[]\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "            elif isinstance(reasoning_step, ActionReasoningStep):\n",
    "                tool_name = reasoning_step.action\n",
    "                tool_args = reasoning_step.action_input\n",
    "                return ToolCallEvent(\n",
    "                    tool_calls=[\n",
    "                        ToolSelection(\n",
    "                            tool_id=\"fake\",\n",
    "                            tool_name=tool_name,\n",
    "                            tool_kwargs=tool_args,\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "        except Exception as e:\n",
    "            (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                ObservationReasoningStep(\n",
    "                    observation=f\"There was an error in parsing my reasoning: {e}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # if no tool calls or final response, iterate again\n",
    "        return PrepEvent()\n",
    "\n",
    "    @step\n",
    "    async def handle_tool_calls(\n",
    "        self, ctx: Context, ev: ToolCallEvent\n",
    "    ) -> PrepEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            if not tool:\n",
    "                (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                    ObservationReasoningStep(\n",
    "                        observation=f\"Tool {tool_call.tool_name} does not exist\"\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                self.sources.append(tool_output)\n",
    "                (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                    ObservationReasoningStep(observation=tool_output.content)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                (await ctx.get(\"current_reasoning\", default=[])).append(\n",
    "                    ObservationReasoningStep(\n",
    "                        observation=f\"Error calling tool {tool.metadata.get_name()}: {e}\"\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # prep the next iteraiton\n",
    "        return PrepEvent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WikiSearchResult(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "\n",
    "class WikiArticle(BaseModel):\n",
    "    title: str\n",
    "    content: str\n",
    "    url: str\n",
    "    \n",
    "def wikipedia_similar_articles(query: str) -> list[Dict[str,str]]:\n",
    "    \"\"\"Search Wikipedia for articles similar to the given query and return titles and URLs.\"\"\"\n",
    "    search_results = wikipedia.search(query, results=5)\n",
    "    result_list = []\n",
    "    for result in search_results:\n",
    "        try:\n",
    "            page = wikipedia.page(result)\n",
    "            result_list.append(WikiSearchResult(title=page.title, url=page.url))\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            # Handle disambiguation pages by logging or ignoring\n",
    "            print(f\"Disambiguation page: {e.options}\")\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            print(f\"PageError: {result}\")\n",
    "    return result_list\n",
    "\n",
    "\n",
    "\n",
    "def wikipedia_full_article(query: str) -> Dict[str,str]:\n",
    "    \"\"\"Fetch the full Wikipedia article for the given query.\"\"\"\n",
    "    try:\n",
    "        page = wikipedia.page(query)\n",
    "        return WikiArticle(title=page.title, content=page.content, url=page.url)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # Handle disambiguation pages\n",
    "        print(f\"Disambiguation page: {e.options}\")\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"PageError: {query}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Wrap these functions in a tool\n",
    "similar_articles_tool = FunctionTool.from_defaults(fn=wikipedia_similar_articles)\n",
    "full_article_tool = FunctionTool.from_defaults(fn=wikipedia_full_article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "agent = ReActAgent(\n",
    "    llm=llm, tools=[similar_articles_tool, full_article_tool], timeout=120, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b2dffc6b7c4edba7f027d44ecd8c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step new_user_msg\n",
      "Step new_user_msg produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event StopEvent\n",
      "Running step new_user_msg\n",
      "Step new_user_msg produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event StopEvent\n",
      "Running step new_user_msg\n",
      "Step new_user_msg produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event StopEvent\n",
      "Running step new_user_msg\n",
      "Step new_user_msg produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "Step handle_tool_calls produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event StopEvent\n",
      "Running step new_user_msg\n",
      "Step new_user_msg produced event PrepEvent\n",
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is the president?</td>\n",
       "      <td>The current president of the United States is ...</td>\n",
       "      <td>[Title: President of the United States, Conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who scored the most goals in the European Cham...</td>\n",
       "      <td>The top goalscorer in the UEFA Champions Leagu...</td>\n",
       "      <td>[Title: List of UEFA Champions League top scor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>articles similar to the article on 'Philosophy'?</td>\n",
       "      <td>Here are some articles similar to the article ...</td>\n",
       "      <td>[Title: Philosophy, URL: https://en.wikipedia....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the school of athens</td>\n",
       "      <td>\"The School of Athens\" is a fresco by the Ital...</td>\n",
       "      <td>[Title: The School of Athens, Content: The Sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>css flex</td>\n",
       "      <td>CSS Flexbox (Flexible Box Layout) is a layout ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                              Who is the president?   \n",
       "1  Who scored the most goals in the European Cham...   \n",
       "2   articles similar to the article on 'Philosophy'?   \n",
       "3                               the school of athens   \n",
       "4                                           css flex   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The current president of the United States is ...   \n",
       "1  The top goalscorer in the UEFA Champions Leagu...   \n",
       "2  Here are some articles similar to the article ...   \n",
       "3  \"The School of Athens\" is a fresco by the Ital...   \n",
       "4  CSS Flexbox (Flexible Box Layout) is a layout ...   \n",
       "\n",
       "                                            contexts  \n",
       "0  [Title: President of the United States, Conten...  \n",
       "1  [Title: List of UEFA Champions League top scor...  \n",
       "2  [Title: Philosophy, URL: https://en.wikipedia....  \n",
       "3  [Title: The School of Athens, Content: The Sch...  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from typing import List, Union\n",
    "# for normal python\n",
    "# import asyncio\n",
    "# for jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "agent.memory.reset()\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "def get_context(response) -> List[str]:\n",
    "    content = []  \n",
    "    for source in response['sources']:\n",
    "        tool_output = source.raw_output\n",
    "        if isinstance(tool_output, list) and all(isinstance(item, WikiSearchResult) for item in tool_output):\n",
    "            for result in tool_output:\n",
    "                content.append(f\"Title: {result.title}, URL: {result.url}\")\n",
    "        elif isinstance(tool_output, WikiArticle):\n",
    "            content.append(f\"Title: {tool_output.title}, Content: {tool_output.content}, URL: {tool_output.url}\")\n",
    "    return content\n",
    "\n",
    "async def generate_response(agent, question) -> Dict[str, Union[str, List[str]]]:\n",
    "    response = await agent.run(input=question)\n",
    "    return {\n",
    "        \"answer\": response['response'],\n",
    "        \"contexts\": get_context(response),\n",
    "    }\n",
    "\n",
    "async def generate_ragas_dataset(agent, test_df):\n",
    "    test_questions = test_df[\"query\"].values\n",
    "    responses = []\n",
    "    \n",
    "    for question in tqdm(test_questions):\n",
    "        response = await generate_response(agent, question)  # Process each question serially\n",
    "        responses.append(response)\n",
    "    \n",
    "    dataset_dict = {\n",
    "        \"question\": test_questions,\n",
    "        \"answer\": [response[\"answer\"] for response in responses],\n",
    "        \"contexts\": [response[\"contexts\"] for response in responses],  # Store contexts separately for each question\n",
    "    }\n",
    "    ds = Dataset.from_dict(dataset_dict)\n",
    "    return ds\n",
    "\n",
    "# limit to first 5 rows for testing\n",
    "with using_project(\"llama-index\"):\n",
    "    ragas_eval_dataset = await generate_ragas_dataset(agent, test_df.head(5))\n",
    "\n",
    "ragas_evals_df = ragas_eval_dataset.to_pandas()\n",
    "ragas_evals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleSpanProcessor' object has no attribute 'client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mphoenix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_qa_with_reference\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mphoenix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mspan_phoenix_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# dataset containing span data for evaluation with Ragas\u001b[39;00m\n\u001b[1;32m      6\u001b[0m spans_dataframe \u001b[38;5;241m=\u001b[39m get_qa_with_reference(client, project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleSpanProcessor' object has no attribute 'client'"
     ]
    }
   ],
   "source": [
    "from phoenix.session.evaluation import get_qa_with_reference\n",
    "import phoenix as px\n",
    "\n",
    "client = span_phoenix_processor.client\n",
    "# dataset containing span data for evaluation with Ragas\n",
    "spans_dataframe = get_qa_with_reference(client, project_name=\"llama-index\")\n",
    "print(spans_dataframe.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682352538e444e19b8c45bdfff77f232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    ")\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    dataset=ragas_eval_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],\n",
    ")\n",
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   faithfulness  answer_relevancy\n",
      "0           1.0          0.886055\n",
      "1           0.0          0.936000\n",
      "2           NaN          0.979653\n",
      "3           1.0          0.861893\n",
      "4           1.0          0.847856\n"
     ]
    }
   ],
   "source": [
    "print(eval_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace.dsl.helpers import SpanQuery\n",
    "\n",
    "client = px.Client()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
